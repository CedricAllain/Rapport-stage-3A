\subsection{EM-based algorithm}

As mentioned above, we would like to fit a model between an atom $k$ and a specific type of stimulus $p$ that would explain the activations of the atom, assuming that the stimulus $p$ might \say{drive} the atom $k$.
It has been hypothesised that the activations of the atom $k$, when driven by the stimulus $p$ follow a temporal point process of intensity $\lambda_{k,p}$ as described in Equation~\eqref{eq:model_intensity}.
To \say{fit} the model simply means that we need to find optimal values of the parameters of the intensity, namely $\mu_k, \alpha_{k,p}, m_{k,p}$ and $\sigma_{k,p}$, that minimise a certain metric, namely the negative log-likelihood.
A way to find such optimal values is by doing a EM-based algorithm, inspired by such algorithm described in \citep{lewis2011nonparametric, xu2016learning} for similar problematic but with different approaches (Hawkes processes, exponential kernels, etc.).

Computations of such an algorithm are detailed below, after a rewriting a the kernel formula, in order to simplify further computations.
In addition, we present later on in Section~\ref{parameters_initialisation} two methods to initialise the parameters, a random one and a \say{smart} one.

\subsubsection{Computations of the coefficients update}

\paragraph{Rewriting of the kernel $\kappa$}
\begin{align*}
    \kappa(x ; m, \sigma, a, b) &= \frac{1}{\sigma} \frac{\phi\pars{\frac{x-m}{\sigma}}}{\Phi\pars{\frac{b-m}{\sigma}} - \Phi\pars{\frac{a-m}{\sigma}}} \1[a\leq x\leq b] \\
    &= \frac{1}{\sigma} \frac{\e{-\frac{1}{2} \frac{\pars{x-m}^2}{\sigma^2}}}{\integ{\frac{a-m}{\sigma}}[\frac{b-m}{\sigma}]{\e{\frac{-t^2}{2}}}{t}} \1[a\leq x\leq b] \\
    &= \frac{\e{-\frac{1}{2}\frac{\pars{x-m}^2}{\sigma^2}}}{\integ{a}[b]{\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}{u}} \1[a\leq x\leq b], \quad t = \frac{u-m}{\sigma} \\
    &= \frac{\e{-\frac{1}{2}\frac{\pars{x-m}^2}{\sigma^2}}}{C\pars{m,\sigma,a,b}} \1[a\leq x\leq b]
\end{align*}
where 
\begin{align*}
    C\pars{m,\sigma,a,b} &\coloneqq \integ{a}[b]{\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}{u} \\
    &= \sigma\sqrt{2\pi}\pars{\Phi\pars{\frac{b-m}{\sigma}} - \Phi\pars{\frac{a-m}{\sigma}}}
\end{align*}
and similarly, we denote by subscripts the partials derivatives:
\begin{align*}
    C_m\pars{m,\sigma,a,b} &\coloneqq \partiald{}{m} C\pars{m,\sigma,a,b} \\
    &= \integ{a}[b]{\frac{u-m}{\sigma^2}\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}{u} \\
    &= \bracks{-\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}_{a}^b \\
    &= \e{-\frac{1}{2}\frac{\pars{a-m}^2}{\sigma^2}} - \e{-\frac{1}{2}\frac{\pars{b-m}^2}{\sigma^2}}
\end{align*}
and 
\begin{align*}
    C_\sigma\pars{m,\sigma,a,b} &\coloneqq \partiald{}{\sigma} C\pars{m,\sigma,a,b} \\
    &= \integ{a}[b]{\frac{(u-m)^2}{\sigma^3}\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}{u} \\
    &= \bracks{-\frac{u-m}{\sigma}\e{-\frac{\pars{u-m}^2}{2\sigma^2}}}_{a}^b + \frac{1}{\sigma} \integ{a}[b]{\e{-\frac{1}{2}\frac{\pars{u-m}^2}{\sigma^2}}}{u} \\
    &= \frac{a-m}{\sigma}\e{-\frac{\pars{a-m}^2}{2\sigma^2}} - \frac{b-m}{\sigma}\e{-\frac{\pars{b-m}^2}{2\sigma^2}} + \frac{1}{\sigma}C\pars{m,\sigma,a,b}
\end{align*}

\paragraph{Negative log-likelihood}
From Equation~\eqref{eq:general_likelihood}, we can define the negative log-likelihood adapted for our specific problem:
\begin{equation}
    \mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} \coloneqq -\log L\left(\lambda_k, \mathscr{F}_{T}^{(p)}\right) = \int_{0}^{T} \lambda_{k,p}(s) \mathrm{d} s-\sum_{t\in\mathcal{A}_k} \log \lambda_{k,p}(t)
\end{equation}

Hypothesis \eqref{eq:separated_timestamps} implies that $\forall i=1,\dots,n_p - 1$,
\begin{align*}
    \integ{t_i^{(p)}}[t_{i+1}^{(p)}]{\kappa_{k,p}(s - t_*^{(p)}(s))\1[s \geq t_1^{(p)}]}{s} &= \underbrace{\integ{t_i^{(p)}}[t_i^{(p)}+a]{\kappa_{k,p}(s - t_i^{(p)})}{s}}_{=0} + \underbrace{\integ{t_i^{(p)}+a}[t_i^{(p)}+b]{\kappa_{k,p}(s - t_i^{(p)})}{s}}_{=1} \\
    &+ \underbrace{\integ{t_i^{(p)}+b}[t_{i+1}^{(p)}]{\kappa_{k,p}(s - t_i^{(p)})}{s}}_{=0} \\
    &= 1
\end{align*}
and hypothesis \eqref{eq:long_duration} implies that
\begin{align*}
    \integ{t_{n_p}^{(p)}}[T]{\kappa_{k,p}(s - t_*^{(p)}(s))\1[s \geq t_1^{(p)}]}{s} &= \underbrace{\integ{t_{n_p}^{(p)}}[t_{n_p}^{(p)} + a]{\kappa_{k,p}(s - t_{n_p}^{(p)})}{s}}_{=0} + \underbrace{\integ{t_{n_p}^{(p)}+a}[t_{n_p}^{(p)} + b]{\kappa_{k,p}(s - t_{n_p}^{(p)})}{s}}_{=1} \\
    &+ \underbrace{\integ{t_{n_p}^{(p)}+b}[T]{\kappa_{k,p}(s - t_{n_p}^{(p)})}{s}}_{=0}\\
    &= 1
\end{align*}

Thus,
\begin{align*}
    \integ{0}[T]{\lambda_{k,p}(s)}{s} &= \integ{0}[T]{\mu_k + \alpha_{k,p}\kappa_{k,p}(s - t_*^{(p)}(s))\1[s \geq t_1^{(p)}]}{s} \\
    &= \mu_k T + \alpha_{k,p}\underbrace{\integ{0}[t_1^{(p)}]{ \kappa_{k,p}(s - t_*^{(p)}(s))\1[s \geq t_1^{(p)}]}{s}}_{=0} + \alpha_{k,p}\sum_{i=1}^{n_p -1} \integ{t_i^{(p)}}[t_{i+1}^{(p)}]{\kappa_{k,p}(s - t_i^{(p)})}{s} \\
    &+ \alpha_{k,p}\integ{t_{n_p}^{(p)}}[T]{\kappa_{k,p}(s - t_{n_p}^{(p)})}{s} \\
    &= \mu_k T + \alpha_{k,p}n_p
\end{align*}

Finally,
\begin{equation}
\begin{split}
    \mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} &= \mu_k T + \alpha_{k,p}n_p \\
    &- \sum_{t\in\mathcal{A}_k} \log \pars{\mu_k + \alpha_{k,p}\kappa_{k,p}(t - t_*^{(p)}(t); m_{k,p}, \sigma_{k,p})\1[t \geq t_1^{(p)}]}
\end{split}
\end{equation}

% TODO : calculer la complexité du calcul de la nll, comme fait dans la thèse de Martin

Then the derivatives of the inverse log-likelihood with respect to the parameters are given by
\begin{equation}
    \partiald{}{\mu_k}\mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} = T - \sum_{t\in\mathcal{A}_k}\frac{1}{\lambda_{k,p}(t)}
\end{equation}

\begin{equation}
\begin{split}
    \partiald{}{\alpha_{k,p}}\mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} &= n_p - \sum_{t\in\mathcal{A}_k}\frac{\kappa_{k,p}(t - t_*^{(p)}(t))}{\lambda_{k,p}(t)}\1[t \geq t_1^{(p)}] \\
    &= n_p - \sum_{t\in\mathcal{A}_k, t \geq t_1^{(p)}}\frac{\kappa_{k,p}(t - t_*^{(p)}(t))}{\lambda_{k,p}(t)}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
    \partiald{}{m_{k,p}}\mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} &= -\sum_{t\in\mathcal{A}_k}\frac{\alpha_{k,p}}{\lambda_{k,p}(t)}\partiald{}{m_{k,p}}\kappa_{k,p}(t - t_*^{(p)}(t); m_{k,p}, \sigma_{k,p}, a, b)\1[t \geq t_1^{(p)}] \\
    &= -\sum_{t\in\mathcal{A}_k, t \geq t_1^{(p)}}\frac{\alpha_{k,p}}{\lambda_{k,p}(t)}\partiald{}{m_{k,p}}\kappa_{k,p}(t - t_*^{(p)}(t); m_{k,p}, \sigma_{k,p}, a, b)
\end{split}
\end{equation}
where
\begin{equation}
    \partiald{}{m}\kappa(x ; m, \sigma, a, b) = \pars{\frac{x-m}{\sigma^2} - \frac{C_m\pars{m, \sigma,a,b}}{C\pars{m, \sigma,a,b}}}\kappa(x ; m, \sigma, a, b)
\end{equation}

\begin{equation}
\begin{split}
    \partiald{}{\sigma_{k,p}}\mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} &= -\sum_{t\in\mathcal{A}_k}\frac{\alpha_{k,p}}{\lambda_{k,p}(t)}\partiald{}{\sigma_{k,p}}\kappa_{k,p}(t - t_*^{(p)}(t); m_{k,p}, \sigma_{k,p}, a, b)\1[t \geq t_1^{(p)}] \\
    &= -\sum_{t\in\mathcal{A}_k, t \geq t_1^{(p)}}\frac{\alpha_{k,p}}{\lambda_{k,p}(t)}\partiald{}{\sigma_{k,p}}\kappa_{k,p}(t - t_*^{(p)}(t); m_{k,p}, \sigma_{k,p}, a, b)
\end{split}
\end{equation}
where
\begin{equation}
    \partiald{}{\sigma}\kappa(x ; m, \sigma, a, b) = \pars{\frac{(x-m)^2}{\sigma^3} - \frac{C_\sigma\pars{m, \sigma,a,b}}{C\pars{m, \sigma,a,b}}}\kappa(x ; m, \sigma, a, b)
\end{equation}

To lighten notations, we define $\mathcal{A}_{k,p} = \enstq{t}{t\in\mathcal{A}_k, t \geq t_1^{(p)}}$ as the set of atom $k$'s activation timestamps that occur after the first timestamps of task $p$.
In other words, $\mathcal{A}_{k,p}$ is the set of all atom $k$'s activations that might have been driven by a task of type $p$.

\paragraph{Expectation step}
Let $P_{t,k}$ be the probability that the activation at time $t$ has been triggered by the baseline intensity of atom $k$, and $P_{t,p}$ be that probability that the activation at time $t$ has been triggered by the driver $p$ (the task $p$ in our case).
\begin{equation}
    P_{t,k} = \frac{\mu_k}{\lambda_{k,p}(t)}
\end{equation}
\begin{equation}
    P_{t,p} = \frac{\alpha_{k,p}\kappa_{k,p}\pars{t - t_*^{(p)}(t)}}{\lambda_{k,p}(t)}\1[t \geq t_1^{(p)}]
\end{equation}

\paragraph{Maximisation step} 
In the following, we denote with the upper script the iteration step, e.g., $\theta^{(n)}$ is the value of the parameter $\theta$ at step $n$ of the EM algorithm.
By extension, if $f$ is a function of $\theta_1, \dots, \theta_m$, then we define $f^{(n)}\pars{\theta_1, \dots, \theta_m} \coloneqq f\pars{\theta_1^{(n)}, \dots, \theta_m^{(n)}}$.
Details of the computation are present in Appendix~\ref{annexe:details_em}.

\begin{equation}
    \mu_k^{(n+1)} = \frac{1}{T} \sum_{t\in\mathcal{A}_k} P_{t,k}^{(n)}
\end{equation}

\begin{equation}
    \alpha_{k,p}^{(n+1)} = \frac{1}{n_p} \sum_{t\in\mathcal{A}_{k,p}} P_{t,p}^{(n)}
\end{equation}

\begin{equation}
    m_{k,p}^{(n+1)} = \frac{\sum_{t\in\mathcal{A}_{k,p}} \pars{t - t_*^{(p)}(t)} P_{t,p}^{(n)}}{\sum_{t\in\mathcal{A}_{k,p}} P_{t,p}^{(n)}} - {\sigma_{k,p}^{(n)}}^2 \frac{C_m\pars{m_{k,p}^{(n)}, \sigma_{k,p}^{(n)},a,b}}{C\pars{m_{k,p}^{(n)}, \sigma_{k,p}^{(n)},a,b}}
\end{equation}

\begin{equation}
    \sigma_{k,p}^{(n+1)} = \pars{\frac{C\pars{m_{k,p}^{(n)}, \sigma_{k,p}^{(n)},a,b}}{C_\sigma\pars{m_{k,p}^{(n)}, \sigma_{k,p}^{(n)},a,b}} \frac{\sum_{t\in\mathcal{A}_{k,p}} \pars{t - t_*^{(p)}(t) - m_{k,p}}^2 P_{t,p}^{(n)}}{\sum_{t\in\mathcal{A}_{k,p}}  P_{t,p}^{(n)}}}^{1/3}
\end{equation}

\paragraph{Remark} If $\alpha_{k,p} = 0$, then the intensity is reduced to its baseline, $\lambda_{k,p}(t) = \mu_k$, and then the negative log-likelihood is:
\begin{equation}
    \mathcal{L}_{k,p}\pars{\mu_k, \alpha_{k,p}, m_{k,p}, \sigma_{k,p}} = \mu_k T -\sum_{t\in\mathcal{A}_k} \log \mu_k = \mu_k T - \#\mathcal{A}_k \log \mu_k
\end{equation}
Thus, we can directly compute the maximum likelihood estimator (MLE) for $\mu_k$, as follow:
\begin{equation}
    \mu_k^{(MLE)} = \frac{\#\mathcal{A}_k}{T}
\end{equation}
Note that in that case, we are no longer interested in the possible values of $m_{k,p}$ and $\sigma_{k,p}$.

\subsubsection{Parameters initialisation}\label{parameters_initialisation}

In this section, we will describe two methods on how to initialise the parameters for the EM algorithm, in other words, how to determine $\theta^{(0)} = \pars{\mu^{(0)}, \alpha^{(0), }m^{(0)}, \sigma^{(0)}}$.
\paragraph{Random initialisation}
The first initialisation method is simply done by random sampling the parameters according to a specific uniform distribution:
\begin{itemize}
    \item $\mu^{(0)} \sim \Unif{\intervalleFF{\mu_{min}}{\mu_{max}}}$, e.g. $\mu_{min} = 0.1$ and $\mu_{max} = 5$;
    
    \item $\alpha^{(0)} \sim \Unif{\intervalleFF{\alpha_{min}}{\alpha_{max}}}$, e.g. $\alpha_{min} = 0.1$ and $\alpha_{max} = 5$;
    
    \item $m^{(0)} \sim \Unif{\intervalleFF{a}{b}}$, were $a$ and $b$ are the truncation values of the kernel, Eq.~\eqref{eq:kernel_trunc_norm};
    
    \item $\sigma^{(0)} \sim \Unif{\intervalleFF{\sigma_{min}}{\sigma_{max}}}$, e.g. $\sigma_{min} = 0.01$ and $\sigma_{max} = 1$.
\end{itemize}

\paragraph{Smart initialisation}
Another method, more intuitive, would be to compute the empirical values of those parameters.
We call this method \say{smart initialisation}.
More specifically, the initial values of the parameters are computed as follow:
\begin{itemize}
     \item Let $\mathcal{D}_{k,p} \coloneqq \enstq{t-t_*^{(p)}(t)}{t\in\mathcal{A}_{k,p}} \cap \intervalleFF{a}{b}$ be the set of all empirical delays possibly linked to the task $p$.
    Then, we define $m^{(0)}$ as the mean of the empirical delays:
    \begin{equation}
        m^{(0)} = \frac{1}{\#\mathcal{D}_{k,p}}\sum_{d \in \mathcal{D}_{k,p}} d
    \end{equation}
    where $\# A$ denotes the cardinality of the set $A$.
    
    \item Similarly, we define $\sigma^{(0)}$ as the standard deviation of the empirical delays:
    \begin{equation}
        \sigma^{(0)} = \sqrt{\frac{1}{\#\mathcal{D}_{k,p}}\sum_{d \in \mathcal{D}_{k,p}} \abs{d - m^{(0)}}^2}
    \end{equation}
    
    \item The activations that do not lend on any kernel support come from the baseline intensity, which is fixed, so it is similar to say that those activations are the result of a homogeneous Poisson process.
    Thus, to initialise the baseline value $\mu^{(0)}$, we simply compute the Poisson process parameter's estimator, which is the average number of activation over the process' duration:
    \begin{equation}\label{eq:baseline_init}
    \begin{split}
        \mu^{(0)} &= \frac{\#\braces{\mathcal{A}_k \setminus \enstq{t}{t\in\mathcal{A}_{k,p}, a\leq t-t_*^{(p)}(t)\leq b}}}{T - n_p\pars{b-a}} \\
        &= \frac{\#\mathcal{A}_k - \#\enstq{t}{t\in\mathcal{A}_{k,p}, a\leq t-t_*^{(p)}(t)\leq b}}{T- n_p\pars{b-a}}
    \end{split}
    \end{equation}
    because there can be no duplicates, and where $n_p\pars{b-a}$ is the sum of all kernels support.
    
    \item Finally, we set $\alpha^{(0)}$ as follow:
    \begin{equation}
    \alpha^{(0)} = f(p_a)
    \end{equation}
    with
    \begin{equation}
        f(p_a) = - e^{\mu} \ln\pars{\frac{l(\mu) - p_a}{l(\mu)  - p_s}}
    \end{equation}
    and
    \begin{equation}
        l(\mu) = \pars{\frac{e^{\mu}-1}{5} + \frac{1}{1-p_s}}^{-1} + p_s
    \end{equation}
    where
    \begin{equation}
        p_a \coloneqq \frac{\#\enstq{t}{t\in\mathcal{A}_{k,p}, a\leq t-t_*^{(p)}(t)\leq b}}{\#\mathcal{A}_k}
    \end{equation}
    is the proportion of activations that lend in a kernel support, and 
    \begin{equation}
        p_s \coloneqq \frac{n_p\pars{b-a}}{T}
    \end{equation}
    is the ratio of all the kernels' support time over the total duration.
    
    More details on the role of the $\alpha$ coefficient and how its initialisation function was determined are presented in Appendix~\ref{app:role_alpha_coef}.
    
\end{itemize}




